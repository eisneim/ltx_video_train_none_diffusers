{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb18ca04-0995-4806-8387-cfc4b53d317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PrecomputedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "206fd82e-2469-4a4f-8ae3-7fcae99aba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eisneim/miniforge3/envs/_learn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-29 14:34:22.171430: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-29 14:34:22.348508: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-29 14:34:23.694146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, random, math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from diffusers.utils import export_to_video\n",
    "from diffusers.video_processor import VideoProcessor\n",
    "\n",
    "from ltx_video.models.autoencoders.vae_encode import (\n",
    "    get_vae_size_scale_factor,\n",
    "    latent_to_pixel_coords,\n",
    "    vae_decode,\n",
    "    # vae_encode,\n",
    ")\n",
    "\n",
    "from ltx_video_lora import *\n",
    "device = \"cuda\"\n",
    "dtype = torch.bfloat16\n",
    "# ------------------- \n",
    "\n",
    "vae = load_latent_models()[\"vae\"].to(device, dtype=dtype)\n",
    "\n",
    "# def _unpack_latents(\n",
    "#         latents: torch.Tensor, num_frames: int, height: int, width: int, patch_size: int = 1, patch_size_t: int = 1\n",
    "#     ) -> torch.Tensor:\n",
    "#     batch_size = latents.size(0)\n",
    "#     latents = latents.reshape(batch_size, num_frames, height, width, -1, patch_size_t, patch_size, patch_size)\n",
    "#     latents = latents.permute(0, 4, 1, 5, 2, 6, 3, 7).flatten(6, 7).flatten(4, 5).flatten(2, 3)\n",
    "#     return latents\n",
    "\n",
    "def _normalize_latents(\n",
    "    latents: torch.Tensor, latents_mean: torch.Tensor, latents_std: torch.Tensor, scaling_factor: float = 1.0,\n",
    "    reverse=False,\n",
    ") -> torch.Tensor:\n",
    "    # Normalize latents across the channel dimension [B, C, F, H, W]\n",
    "    latents_mean = latents_mean.view(1, -1, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "    latents_std = latents_std.view(1, -1, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "    if not reverse:\n",
    "        latents = (latents - latents_mean) * scaling_factor / latents_std\n",
    "    else:\n",
    "        latents = latents * latents_std / scaling_factor + latents_mean\n",
    "    return latents\n",
    "\n",
    "\n",
    "dest_dir = \"/home/eisneim/www/ml/_video_gen/LTX-Video-3-22/data/images/dataset_extracted\"\n",
    "dataset_dirs = [\n",
    "    '/media/eisneim/4T/ltx_data_49_blured/game_p11_49x1024x576',\n",
    "    # '/media/eisneim/4T/ltx_data_49_blured/game_p10_blured_49x1024x576', \n",
    "    # '/media/eisneim/4T/ltx_data_49_blured/game_p9_49x1024x576', \n",
    "    # '/media/eisneim/4T/ltx_data_49_blured/game_p8_49x1024x576', \n",
    "    # '/media/eisneim/4T/ltx_data_49_blured/3dgs_game_1-6_49x1024x576', \n",
    "    # '/media/eisneim/4T/ltx_data_49_blured/game_p7_49x1024x576', \n",
    "    # \"/media/eisneim/4T/ltx_data_81x1024x576/game_p13_81x1024x576\",\n",
    "]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c4066e-20f7-4c31-8e71-21a7a68b1bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached videos: 2066\n",
      "torch.Size([1, 128, 1, 18, 32])\n",
      "image decode torch.Size([1, 3, 1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "dataset = PrecomputedDataset(dataset_dirs[0])\n",
    "\n",
    "timestep = torch.tensor([0.05], device=device, dtype=dtype)\n",
    "# timestep = None\n",
    "is_video = False\n",
    "\n",
    "\n",
    "for idx, data in enumerate(dataset):\n",
    "    if idx > 0:\n",
    "        break\n",
    "\n",
    "    _, first_frame, _, _, caption, info = data\n",
    "    frame = unpack_latents(first_frame.unsqueeze(0).to(device, dtype=dtype), 1, info[\"height\"],  info[\"width\"])\n",
    "    frame = _normalize_latents(frame, vae.mean_of_means, vae.std_of_means, reverse=True)\n",
    "    print(frame.shape)\n",
    "    with torch.no_grad():\n",
    "        # video =  vae.decode(lt, timestep, return_dict=False)[0]\n",
    "        image = vae_decode(\n",
    "            frame, vae, is_video,\n",
    "            vae_per_channel_normalize=False,\n",
    "            timestep=timestep,\n",
    "        )\n",
    "        print(\"image decode\", image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c901aca3-f1ec-4435-a8e1-1d7c32365e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11392"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [279, 1742, 1783, 1041, 957, 2041, 3549]\n",
    "sum(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5de65517-9814-421f-ad00-0d6f12cb6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1743230953'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2cab21c-93bd-4129-9e66-3802b5669fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640/32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24b5ff52-f4ec-49cd-a970-774edee740ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "360/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c075c9d-c775-4f47-bfa1-2b45940bae78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66e8118b-4d65-44ec-8607-0b55e355bf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f81df32-14e6-4972-839f-d5c09cc04d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.8125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "672 / 16 * 9 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b250c5d-a9af-4ad7-bf34-67eca5fcff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08a2b76a-f61d-485e-899d-a0ddc3e98dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8154"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = [1742, 279, 1229, 4904]\n",
    "sum(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "112adebe-8a37-46e3-8d08-9eb1c99ddd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5e-06,\n",
       " 1e-05,\n",
       " 1.5000000000000002e-05,\n",
       " 2e-05,\n",
       " 2.5e-05,\n",
       " 3.0000000000000004e-05,\n",
       " 3.5000000000000004e-05,\n",
       " 4e-05,\n",
       " 4.5e-05,\n",
       " 5e-05,\n",
       " 5.5e-05,\n",
       " 6.000000000000001e-05,\n",
       " 6.500000000000001e-05,\n",
       " 7.000000000000001e-05,\n",
       " 7.500000000000001e-05]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x * 5e-6 for x in range(1, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0af4bd86-3c2b-43dc-8553-53db1fb38db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 /3 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8edbffc2-d543-489a-8d6e-3fc0e96bbd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bb636-7af8-4dd9-82c8-706fcbd91602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_learn",
   "language": "python",
   "name": "_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
